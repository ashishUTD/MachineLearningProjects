# -*- coding: utf-8 -*-
"""ML_Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUDJmiLCDMol1LczEPXgb9Jx2DmmxkaO
"""

#Importing Libraries 
import pandas as pd 
import json
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import preprocessing
plt.rcParams['figure.figsize'] = [12, 7]
from math import sqrt
from sklearn.model_selection import GridSearchCV
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

def parse_json_object_into_column(series):    
    if isinstance(series[0], str):
        return pd.DataFrame(json.loads(s) for s in series)
    return pd.DataFrame(s for s in series)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Google Analytics/train.csv', engine='python')

# Following are the json columns in the data. We will parse it to make eack key, value pair as unique column
jsonColumn = ['device', 'geoNetwork', 'totals', 'trafficSource']
nestedJsonColumn = ['adwordsClickInfo']

# parsing json into unique column
df = df.join(parse_json_object_into_column(df[col_name]) for col_name in jsonColumn).drop(jsonColumn, axis=1)
df = df.join(parse_json_object_into_column(df[nestedJsonColumn[0]])).drop(nestedJsonColumn, axis=1)

newdf = df.drop(columns=['targetingCriteria', 'campaignCode', 'sessionId'])

# Dropping all the columns with constant value
constant_columns = [col for col in newdf.columns if newdf[col].nunique(dropna=False)==1 ]
newdf = newdf.drop(constant_columns, axis=1)

# Filling NaN values with zero using dataframe fillna method
for col in newdf.columns:
  newdf[col].fillna(newdf[col].mode()[0],inplace = True)

#Changing below colunms to float
numerical_cols = ["hits", "pageviews", "visitNumber", "visitStartTime", 'bounces',  'newVisits', 'transactionRevenue']    
for col in numerical_cols:
    newdf[col] = newdf[col].astype(float)

# Applying label encoder to categorical data
#categorical_columns = newdf.select_dtypes(include ='object').columns
categorical_columns = ["channelGrouping", "browser", 
            "deviceCategory", "operatingSystem", 
            "city", "continent", 
            "country", "metro",
            "networkDomain", "region", 
            "subContinent", "adContent", 
            "adNetworkType", 
            "gclId", 
            "page", 
            "slot", "campaign",
            "keyword", "medium", 
            "referralPath", "source",
            'isVideoAd', 'isTrueDirect']
numerical_cols = ["hits", "pageviews", "visitNumber", "visitStartTime", 'bounces',  'newVisits']

for col in categorical_columns:
    label = preprocessing.LabelEncoder()
    label.fit(list(newdf[col].values.astype('str')))
    newdf[col] = label.transform(list(newdf[col].values.astype('str')))

testdf = newdf.copy()

f , ax = plt.subplots(figsize = (14,12))
plt.title('Correlation of Features- HeatMap',y=1,size=16)
sns.heatmap(testdf.corr(),square = True,  vmax=0.8)

numerical_cols = ["hits", "pageviews", "visitNumber", "visitStartTime", 'bounces',  'newVisits']

X = newdf.drop(columns=['transactionRevenue'])
y = np.log1p(newdf['transactionRevenue'].values)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train[categorical_columns + numerical_cols]

X_test = X_test[categorical_columns + numerical_cols]

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train_scalar = scaler.transform(X_train)
X_test_scalar = scaler.transform(X_test)

from sklearn.decomposition import PCA
pca = PCA(n_components=20)
principalComponents = pca.fit_transform(X_train_scalar)
print(pca.explained_variance_)
print(pca.explained_variance_ratio_) 

transformed_test = pca.transform(X_test_scalar)

# With PCA
def LinearRegression():
  tuned_parameters = {}
  clf = GridSearchCV(linear_model.LinearRegression(), tuned_parameters, cv=5,
                       scoring='r2')
  clf.fit(principalComponents, y_train)
  
  y_true, y_pred = y_test, clf.predict(transformed_test)
  errors_lr = abs(y_true - y_test)
  print('Mean Absolute Error LR:', round(np.mean(errors_lr), 2))

  
  print('Variance score: %.2f' % r2_score(y_true, y_pred))
  print('Root Mean squared error: %.2f' % sqrt(mean_squared_error(y_true, y_pred)))

LinearRegression()

!pip install lightgbm

import lightgbm as lgb

# Without PCA
def LinearRegression():
  tuned_parameters = {}
  clf = GridSearchCV(linear_model.LinearRegression(), tuned_parameters, cv=5,
                       scoring='r2')
  clf.fit(X_train, y_train)
  
  y_true, y_pred = y_test, clf.predict(X_test)
  errors_lr = abs(y_true - y_test)
  print('Mean Absolute Error LR:', round(np.mean(errors_lr), 2))

  
  print('Variance score: %.2f' % r2_score(y_true, y_pred))
  print('Root Mean squared error: %.2f' % sqrt(mean_squared_error(y_true, y_pred)))

LinearRegression()

#With PCA
def LGBModel(principalComponents, y_train, transformed_test, y_test):
    params = {
        "objective" : "regression",
        "metric" : "rmse", 
        "num_leaves" : 35,
        "min_child_samples" : 90,
        "learning_rate" : 0.11,
        "bagging_fraction" : 0.68,
        "feature_fraction" : 0.52,
        "bagging_frequency" : 6,
        "bagging_seed" : 2020,
        "verbosity" : -1
    }
    
    lgb_train = lgb.Dataset(principalComponents, label=y_train)
    model = lgb.train(params, lgb_train, 1000, verbose_eval=100)
    
    predict_test_lgm = model.predict(transformed_test, num_iteration=model.best_iteration)
    return predict_test_lgm, model

predict_test_lgm, model = LGBModel(principalComponents, y_train, transformed_test, y_test)
errors_lgm = abs(predict_test_lgm - y_test)
print('Mean Absolute Error lgm:', round(np.mean(errors_lgm), 2))
print('Variance score: %.2f' % r2_score(y_test, predict_test_lgm))
print('Root Mean squared error: %.2f' % sqrt(mean_squared_error(y_test, predict_test_lgm)))

# Without PCA
def LGBModel(X_train, y_train, X_test, y_test):
    params = {
        "objective" : "regression",
        "metric" : "rmse", 
        "num_leaves" : 35,
        "min_child_samples" : 90,
        "learning_rate" : 0.11,
        "bagging_fraction" : 0.68,
        "feature_fraction" : 0.52,
        "bagging_frequency" : 6,
        "bagging_seed" : 2020,
        "verbosity" : -1
    }
    
    lgb_train = lgb.Dataset(X_train, label=y_train)
    model = lgb.train(params, lgb_train, 1000, verbose_eval=100)
    
    predict_test_lgm = model.predict(X_test, num_iteration=model.best_iteration)
    return predict_test_lgm, model

predict_test_lgm, model = LGBModel(X_train, y_train, X_test, y_test)
errors_lgm = abs(predict_test_lgm - y_test)
print('Mean Absolute Error lgm:', round(np.mean(errors_lgm), 2))
print('Variance score: %.2f' % r2_score(y_test, predict_test_lgm))
print('Root Mean squared error: %.2f' % sqrt(mean_squared_error(y_test, predict_test_lgm)))

!pip install xgboost

# With PCA
import xgboost as xgb
from xgboost import XGBRegressor
xgb_params = {
        'n_estimators': 1000,
        'objective': 'reg:linear',
        'booster': 'gbtree',
        'learning_rate': 0.02,
        'max_depth': 25,
        'min_child_weight': 60,
        'gamma' : 1.5,
        'subsample': 0.69,
        'colsample_bytree': 0.058,
        'colsample_bylevel': 0.57,
        'n_jobs': -1,
        'predictor' : 'gpu_predictor'
        
    }
    
fit_params = {
        'early_stopping_rounds': 17,
        'eval_metric': 'rmse',
        'verbose': False
    }
classifier = XGBRegressor(xgb_params=xgb_params,fit_params=fit_params,cv=10)   
classifier.fit(principalComponents, y_train)
y_pred_xg = classifier.predict(transformed_test)
errors_xg = abs(y_pred_xg - y_test)
print('Mean Absolute Error XGB:', round(np.mean(errors_xg), 2))
print('Variance score: %.2f' % r2_score(y_test, y_pred_xg))
print('Root Mean squared error: %.2f' % sqrt(mean_squared_error(y_test, y_pred_xg)))

# Without PCA
import xgboost as xgb
from xgboost import XGBRegressor
xgb_params = {
        'n_estimators': 1000,
        'objective': 'reg:linear',
        'booster': 'gbtree',
        'learning_rate': 0.02,
        'max_depth': 25,
        'min_child_weight': 60,
        'gamma' : 1.5,
        'subsample': 0.69,
        'colsample_bytree': 0.058,
        'colsample_bylevel': 0.57,
        'n_jobs': -1,
        'predictor' : 'gpu_predictor'
        
    }
    
fit_params = {
        'early_stopping_rounds': 17,
        'eval_metric': 'rmse',
        'verbose': False
    }
classifier = XGBRegressor(xgb_params=xgb_params,fit_params=fit_params,cv=10)   
classifier.fit(X_train, y_train)
y_pred_xg = classifier.predict(X_test)
errors_xg = abs(y_pred_xg - y_test)
print('Mean Absolute Error XGB:', round(np.mean(errors_xg), 2))
print('Variance score: %.2f' % r2_score(y_test, y_pred_xg))
print('Root Mean squared error: %.2f' % sqrt(mean_squared_error(y_test, y_pred_xg)))

